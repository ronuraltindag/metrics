# Hypothesis testing

```{css, echo=FALSE}
.badCode {
background-color: #ffffcf;
  color: black;
}
```

```{r , include=FALSE}

need <- c('glue', 'dplyr','readxl', 'haven', 'tidyr', 'tufte', 'formatR','knitr','rmdformats','ggplot2','broom')

have <- need %in% rownames(installed.packages()) 
if(any(!have)) install.packages(need[!have]) 
invisible(lapply(need, library, character.only=T)) 

knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
#change path to whereever you place the models
# script_folder = dirname(rstudioapi::getSourceEditorContext()$path)
# setwd(glue('{script_folder}'))

birthday<-"01011990" #january 1, 1990
set.seed(as.integer(birthday))
```

## Goals (previous)

* Simulate a data-generating process (DGP) from scratch, given the population parameters 
* Show how to sample from the DGP numerous times, with each sample generating a sample estimate
* Show that these sets of sampling estimates follow a distribution

* **Note** : Code chunks with STUDENTS are for intented to teach students, CHALLENGE are challenging but students with some programming skills can easily learn, PROF are for students with advanced programming skills and instructors.

## Goals (current)

* Show a sampling distribution when true $\beta_1 = 0$.

## Overview

* We follow the univariate DGP from before. We then loop our process to generate many samples, each with its own estimate of $\beta_1$. We then graph the distribution of the estimates and look at their properties.
* The only change here is to set $\beta_1= 0$ in the DGP.

## Generating data from a known linear univariate DGP

```{r}
n <- 1000000
u<-rnorm(n, 0, sqrt(9))
x<-rnorm(n, 10,sqrt(1))
beta0<-1
beta1<-0
y<-beta0+beta1*x+u
df<-data.frame(y,x,u) 
head(df)
``` 

## Sampling from the population 

* Next, we will take 1,000 samples of 1,000 observation from the population data, with each sample yielding us a $\hat{\beta_0}$ and $\hat{\beta_1}$. 
```{r}

b_0<-c()
b_1<-c()
for(i in 1:1000){ 
sff<- df[sample(nrow(df), 1000, replace=FALSE), ]
estimates<-lm(y~x, sff)
b_0[i]<-coef(estimates)[1]
b_1[i]<-coef(estimates)[2]
}
coeff<-data.frame(b_0, b_1)
head(coeff)


ggplot(coeff, aes(x=b_0)) + 
  geom_histogram(color="black", fill="pink", binwidth = 0.05)+
   ggtitle('TKtitle') 

ggplot(coeff, aes(x=b_1)) + 
  geom_histogram(color="black", fill="pink", binwidth = 0.05)+
   ggtitle('TKtitle') 

```

## Properties of the empirical simulations

```{r}
mean(coeff$b_0)
mean(coeff$b_1)

var(coeff$b_0)
var(coeff$b_1)

```

### What values of B1 hat define the cutoff for the 5% of observations in either tail?
```{r}
quantile(coeff$b_1, c(.05, .95)) 
```



### Divide all values of B1 hat by the standard deviation of the distribution of b1 hats. 
What value of this transformed B1 hat defines the cutoff for the 5% of observations with the largest absolute value you get in the (transformed) sampling distribution?
```{r}
coeff$b_1_transformed <- coeff$b_1/sd(coeff$b_1)
quantile(coeff$b_1_transformed, c(.05, .95)) 
```


### Randomly sample coefficients from the distribution until you get one that is in the 5% tails. 

* How many tries does it take to get a value "far away" from the true value?
```{r}
tails<-quantile(coeff$b_1, c(.05, .95)) 
sample_beta1<-0 #start by setting this to zero for the while loop
i<-1 #loop counter for print
while (sample_beta1>=tails[1] & sample_beta1<=tails[2]) {
  sample_beta1<-sample(coeff$b_1,1)
  print(paste0('Sample number ',i,": ",sample_beta1))
  i<-i+1
}
```