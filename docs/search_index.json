[
["index.html", "EC282 INTRODUCTION TO ECONOMETRICS Chapter 1 Course Logistics and using R 1.1 Goals 1.2 Course information 1.3 Requirements", " EC282 INTRODUCTION TO ECONOMETRICS Onur Altındağ Last update: 2020-01-06 Chapter 1 Course Logistics and using R 1.1 Goals Course introduction, syllabus, general logistics, etc. Install R and RStudio. Some pep talk on “how to cope with coding frustration”. Best practices relating to code/script development in the RStudio Integrated Development Environment (IDE). Areas of caution and some never-do’s. 1.2 Course information This is an introductory econometrics course that aims to provide you the basic set of tools to conduct data analysis and interpretation based on economic theory. The focus of the course is the theoretical and empirical foundations of statistical inference and modeling. We will study the basic mechanics of multivariate regression, problems related to its implementation and how to solve them. Towards the end of the semester, subject to a time constraint, we will also discuss issues related to in-sample and out-of-sample prediction problems. The course aims to be application oriented and you should develop, to the extend possible, some programming skills in R. At the end of the semester, I expect you to be familar with RStudio interface, basic data manipulation, obtaining and interpreting the sample statistics, conduct meaningful regression analysis and prediction. Importantly, we will not talk much about the elephant in the room: causal interpretation of any empirical analysis, which is left for the next metrics class that you will take. 1.3 Requirements Textbook: I will mainly use this web page and upload my hand to provide you some material and upload my hand-written lecture notes as well. There are two main textbooks that I heavily rely on to explain the theoretical framework: Studenmund’s Using Econometrics: A Practical Guide. Introductory Econometrics: A Modern Approach. Feel free to purchase an older edition or rent them. The "],
["introduction.html", "Chapter 2 Introduction", " Chapter 2 Introduction .badCode { background-color: #ffffcf; color: black; } "],
["introduction-1.html", "Chapter 3 Introduction 3.1 Goals 3.2 R and RStudio", " Chapter 3 Introduction 3.1 Goals Install R and RStudio. TRYING NEW . Best practices relating to code/script development in the RStudio Integrated Development Environment (IDE). Areas of caution and some never-do’s. 3.2 R and RStudio 3.2.1 Notes I assume a basic familiarity with R and RStudio, or expect your effort to gain that familiarity around the same time as this lesson is taught. 3.2.2 Install R and R studio Here are the instructions for installing R and RStudio on your Windows or Mac desktop. Skip the third part and do not install “SDSFoundations Package” 3.2.3 Basic rules and best practices All files should exist in a local folder that syncs to a cloud-storage service. No file you ever work on should be at risk of being lost if your computer ceases to function or be in your possesion. NEVER place any file on “downloads” or “desktop” folders. Get a free cloud-storage service with a desktop application that syncs to a cloud-storage service. I like the Dropbox desktop app but feel free to choose any other service. You don’t need a lot of space so make sure that you don’t pay. Under the Dropbox folder, create a designated folder for this course such as EC282. All subfolders under EC282 and files in them should have unique and descriptive construction: DON’T use spaces in file or folder names. Here is an example of a folder structure that might work for a student in this class: EC282 │ │ └───Course_docs │ │ SyllabusEC282.pdf │ │ LectureNotes.pdf └───Assignments │ └───Assignment1 │ │ dataset1name.Rda │ │ Lastname_Firstname_Assignment1_EC282.R │ └───Assignment2 │ │ dataset2name.Rda │ │ Lastname_Firstname_Assignment1_EC282.R │ └───Assignment3 │ │ dataset3name.Rda │ │ Lastname_Firstname_Assignment3_EC282.R │ └───Assignment4 │ │ dataset4name.Rda │ │ Lastname_Firstname_Assignment4_EC282.R │ │ ... └───Exams │ └───Midterm1 │ │ Midterm1Review.pdf │ │ Midterm1Review_myanswers.docx | | ... │ At the beginning of any R script, you should have a standard header that you use across all scripts tha clears the workspace, loads/installs packages as necessary, sets the working directory, etc. Here is an example that you can copy paste to the header of any script that you use: ############################################################################### # list the packages we need and loads them, installs them automatically if we don&#39;t have them # add any package that you need to the list need &lt;- c(&#39;glue&#39;, &#39;dplyr&#39;, &#39;ggplot2&#39;,&#39;tidyr&#39;, &#39;ggthemes&#39;,&#39;tidyr&#39;) have &lt;- need %in% rownames(installed.packages()) if(any(!have)) install.packages(need[!have]) invisible(lapply(need, library, character.only=T)) # To set up the working directory getwd() setwd(getwd()) #change getwd() here is you need to set a different working directory #this clears the workspace rm(list = ls()) #this sets the random number generator seed to my birthday for replication set.seed(06061983) ############################################################################### When coding, use relative references to files. Typically, any script will begin looking for files in the working directory. At any time you can type getwd() on your Rstudio console to see the current working directory. The header above automatically sets the working directory to the folder that the R script is included. For example, if you are working on Lastname_Firstname_Assignment1_EC282.R script and need to load file dataset1name.Rda into an object, then you would simply run: load(dataset1name.Rda) However, if you were working in the same .R file, and needed to access dataset2name.Rda, you would need to point the program to a directory outside the current working directory – so, you go up one level, over one folder, and look there: load(../Assignment2/dataset2name.Rda) When learning R, the most important skill that you need to acquire is to be able to google stuff. There is probably not a single R question that you have yet has not been answered on Stack Overflow. "],
["prob-stat-review.html", "Chapter 4 Prob-Stat Review", " Chapter 4 Prob-Stat Review .badCode { background-color: #ffffcf; color: black; } .badCode { background-color: #ffffcf; color: black; } 4.0.1 Goals Briefly review the concepts related to probability theory and statistics that we will frequently use in this class. Briefly review how to infer meaningful information about the population using random samples. 4.0.2 Flipping a coin Task: Define the mutually exclusive outcomes for flipping a coin and flip the coin one time. Answer: ## [1] &quot;heads&quot; 4.0.3 Probability Task: Show that if you repeat the exercise many times (e.g. 1000), the coin will come up heads roughly half of the time. That is P(head) = 0.5 Answer: #define the set of possible outcomes outcome.set &lt;- c(&quot;heads&quot;,&quot;tail&quot;) #flip the coin 1000 times and save it into a list event.outcome &lt;- replicate(1000, sample(outcome.set, 1)) #calculate the number of heads table(event.outcome) ## event.outcome ## heads tail ## 472 528 DIFFICULT answer: lln &lt;- function (X){ outcome.set &lt;- c(&quot;heads&quot;,&quot;tail&quot;) event.outcome &lt;- replicate(X, sample(outcome.set, 1)) y1 &lt;- as.data.frame(event.outcome) %&gt;% mutate(trials=X) return(y1) } out1 &lt;- do.call(rbind.data.frame,lapply(seq(10,1000,10), lln)) %&gt;% group_by(trials, event.outcome) %&gt;% count() %&gt;% mutate(frac=n/trials) g1 &lt;- ggplot(data=out1, aes(x=trials,y=frac, colour=event.outcome)) + geom_segment(aes(x = trials,y = frac, xend = trials, yend = 1-frac), size = 0.3, colour=&#39;#969696&#39;) + geom_point(size=1.5) + geom_hline(yintercept = 0.5, colour=&#39;black&#39;, linetype=&#39;dashed&#39;, size=0.5) + ylim(0.25, 0.75) + theme_fivethirtyeight() + coord_flip() + scale_colour_manual(name=&#39;&#39;, values = c(&#39;#1FA67A&#39;,&#39;red&#39;)) + labs(x = &quot;number of coins flipped&quot;, y = &quot;fraction of heads and tails&quot;) + theme(legend.position=&quot;bottom&quot;) plot(g1) 4.0.4 Expected value and variance Task: Define a random variable X = {0,1} through assigning zero to heads and 1 to tails. Calculate the expected value, variance, and the standard deviation of the random variable X: \\(\\mu=E[X]=\\sum_iX_iP(X_i)\\) and \\(\\sigma^2=E[(X-\\mu)^2]=\\sum_i[(X_i-\\mu)^2 P(X_i)]\\) Answer: #define an empty matrix 2 by 6 df1 &lt;- matrix(NA, nrow = 2, ncol = 6) #transform this to a data set (easier to work with) df1 &lt;- as.data.frame(df1) #change the col names names(df1) &lt;- c(&quot;outcome&quot;, &quot;X&quot;, &quot;P(X)&quot;, &quot;X.P(X)&quot;, &quot;X-E[X]&quot;, &quot;(X-E[X])^2.P(X)&quot;) #define the outcomes df1$outcome &lt;- c(&quot;heads&quot;, &quot;tail&quot;) #define values assigned to each outcome df1$X &lt;- c(0,1) #define the associated probabilities df1$`P(X)` &lt;- c(0.5,0.5) #weight each outcome with the assigned probability df1$`X.P(X)` &lt;- df1$X * df1$`P(X)` #calculate the expexted value mu.x &lt;- sum(df1$`X.P(X)`) #calculate the dev. from mean for each outcome df1$`X-E[X]` &lt;- df1$X - mu.x #weight the squared dev. from mean by the assigned probability df1$`(X-E[X])^2.P(X)` &lt;- (df1$`X-E[X]`)^2 * df1$`P(X)` #calculate the variance var.x &lt;- sum(df1$`(X-E[X])^2.P(X)`) #std. dev sigma.x &lt;- sqrt(var.x) #show the filled data set df1 ## outcome X P(X) X.P(X) X-E[X] (X-E[X])^2.P(X) ## 1 heads 0 0.5 0.0 -0.5 0.125 ## 2 tail 1 0.5 0.5 0.5 0.125 #show the mean, variance, and standard deviation rbind(mu.x,var.x, sigma.x) ## [,1] ## mu.x 0.50 ## var.x 0.25 ## sigma.x 0.50 4.0.5 Law of Large Numbers Formally, the sample average, \\(\\bar{x}\\), will almost surely converge in probablity to \\(\\mu\\) as \\(N \\rightarrow \\infty\\). In practice, here is what it looks like. Task: Define the random variable X={0,1} as previously described. Flip the coin for an increasing number of times, n={1,2,3….,1000}, and calculate the average of each sample. Interpret the expected value. Answer: Law of Large Numbers (LLN) theorem predicts that if an experiment is repeated many times the average outcome should be very close to the expected value, observe how difference between the observed sample average and expected value approach each other as the number of coins that we experiment with increases. Expected value is therefore the long run average of a random variable. flips &lt;- function(n,p,x1,x2) { out1 &lt;- sample(x=c(x1,x2), prob = c(p,1-p), size=n, replace = TRUE) n.sample &lt;- n mean.sample &lt;- sum(out1)/n sd.sample &lt;- sqrt(sum((out1-mean.sample)^2)/n) out2 &lt;- cbind(mean.sample, n.sample, sd.sample) return(out2) } df1 &lt;- do.call(rbind.data.frame,lapply(1:1000, flips,p=0.5,x1=0,x2=1)) p1 &lt;- ggplot(df1, aes(x=n.sample,y=mean.sample)) + theme_tufte() + geom_line() + geom_hline(yintercept = 0.5, colour=&#39;grey&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;number of coins flipped&quot;, y = &quot;outcome mean&quot;) + transition_reveal(n.sample) + view_follow() + xlim(1,1000 ) + ylim(0.0,1.0) animate(p1, nframes=200, fps=5) 4.0.6 Central limit theorem CLT is a fundamental tool in data analysis, there are many versions with similar implications. We will demonstrate a simple one. Formally, assume that you have an identically and independently distributed random variable X. Notice that the shape of its outcome distibution does not matter. You draw a random sample of \\(N\\) observations through repeated experimentation such as \\(\\{X_1,X_2,..,X_n\\}\\). Assume that you are interested in the sample average \\(\\bar{X}=\\dfrac{X_1+X_2+...+X_n}{n}\\). CLT states that, no matter what the initial distribution of X is, the sample average of X can be approximated with a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\dfrac{\\sigma}{\\sqrt{n}}\\) for sufficiently large \\(n\\). Here is how it works. Task: Flip 10 coins using random variable X={0,1} as defined before and calculate the sample average \\(\\bar{X}\\). Answer: #define your sample size=10 n &lt;- 10 #define the number times that you want to repeat the experiment r=1000 r &lt;- 1000 #define the possible outcomes X={0,1} x1 &lt;- 0 x2 &lt;- 1 #define the probabilities for each outcome p1 &lt;- 0.5 p2 &lt;- 0.5 #now run the experiment exp.outcome &lt;- sample(x=c(x1,x2), prob=c(p1,p2), size=10, replace=TRUE) exp.outcome ## [1] 1 1 1 0 1 1 1 1 0 0 mean(exp.outcome) ## [1] 0.7 Task: Repeat the 10-coin experiment many times (e.g. ~ 10,000) and calculate the average of outcomes for each experiment. Graph a frequency histogram of these averages, that is the distribution of \\(\\bar{X}\\). HARD answer: #write a small function f.coins &lt;- function(n) { sample.n &lt;- sample(x=c(0,1), prob=c(0.5,0.5), size=n, replace=TRUE) mean.sample &lt;- mean(sample.n) return(mean.sample) } #now try if it is working f.coins(10) ## [1] 0.7 #replicate 10000 times df1 &lt;- replicate(10000, f.coins(10)) #see if the data looks right head(df1) ## [1] 0.5 0.6 0.4 0.6 0.3 0.6 #now plot hist.df1 &lt;- hist(df1, xlab=&#39;sample average of 10-coins&#39;, ylab=&#39;frequency (over 10000)&#39;,main=&#39;&#39;) Task: Gradually increase your sample size from 10 to 100 and plot the frequency distribution of \\(\\bar{X}\\) with increasing \\(n\\). "],
["generating-population-data-sampling-from-the-population-and-deriving-sample-estimates-of-population-parameters.html", "Chapter 5 Generating (population) data, sampling from the population, and deriving sample estimates of population parameters 5.1 Goals 5.2 Overview 5.3 Generating data from a known linear univariate DGP 5.4 Sampling from the population 5.5 Calculating sample estimates 5.6 Plot the data with the fitted line 5.7 Calculating TSS, ESS, and \\(R^2\\) 5.8 Algebraic properties of the estimator", " Chapter 5 Generating (population) data, sampling from the population, and deriving sample estimates of population parameters .badCode { background-color: #ffffcf; color: black; } 5.1 Goals Simulate a data-generating process (DGP) from scratch, given the population parameters Show how to sample from the DGP to generate sample estimates Understand why these sample estimates can be different from the population values Note : Code chunks with STUDENTS are for intented to teach students, CHALLENGE are challenging but students with some programming skills can easily learn, PROF are for students with advanced programming skills and instructors. 5.2 Overview Data are observed as a result of a data generating process (from hereon, DGP). You can think of a coin flip as a data generating process for a binary outcome (Bernoulli distribution). In practice, we are more often working with data that follow a DGP that is (a) more complex, and (b) unknown. But to build intuition in this course, we will work a lot with simple, known DGPs. 5.3 Generating data from a known linear univariate DGP So let’s perform a data generating process for a dependent variable \\[y\\] which is given by the linear function \\[y = \\beta_0 + \\beta_1 * x + u\\] where \\(\\beta_0\\) is the sum of the 1st and 2nd digit of my birthday, \\(\\beta_1\\) is the sum of the 3rd and 4th digit of my birthday, x is distributed normal with mean of 10 and variance of the first non-zero digit of my birthday, and u is (independently) distributed normal with mean 0 and variance of the last non-zero digit of your birthday. Let’s say the “population” is 1MM observations. So generate all the components, and then multiply and sum them to generate the column for Y, and combine all columns into a single dataframe. n &lt;- 1000000 u&lt;-rnorm(n, 0, sqrt(9)) x&lt;-rnorm(n, 10,sqrt(1)) beta0&lt;-1 beta1&lt;-1 y&lt;-beta0+beta1*x+u df&lt;-data.frame(y,x,u) head(df) ## y x u ## 1 10.708070 10.49637 -0.7882969 ## 2 12.450647 10.45746 0.9931879 ## 3 8.818106 11.68836 -3.8702541 ## 4 9.882678 10.14413 -1.2614506 ## 5 11.508339 10.01319 0.4951523 ## 6 17.616721 10.03691 6.5798110 5.4 Sampling from the population Next, we will take a sample from the population data of 1,000 observation. sample&lt;- df[sample(nrow(df), 1000), ] nrow(sample) ## [1] 1000 5.5 Calculating sample estimates And let’s manually calculate b0 and b1 using the functions sum() and var() in R. ybar&lt;-mean(sample$y) xbar&lt;-mean(sample$x) beta1_hat&lt;-cov(sample$y,sample$x)/var(sample$x) beta0_hat&lt;-ybar-beta1_hat*xbar print(beta0_hat) ## [1] 0.5865453 print(beta1_hat) ## [1] 1.04084 So our sample-derived regression function is \\[y = 0.5865453 + 1.0408402*x + u\\] 5.6 Plot the data with the fitted line sample$predictedvalues &lt;-beta0_hat + beta1_hat*sample$x ggplot(sample, aes(x, y)) + geom_point() + geom_line(aes(x, predictedvalues)) 5.7 Calculating TSS, ESS, and \\(R^2\\) sample$predictedvalues &lt;-beta0_hat + beta1_hat*sample$x sample$residuals &lt;-sample$y-sample$predictedvalues TSS &lt;-sum((sample$y-ybar)^2) ESS &lt;-sum((sample$predictedvalues-ybar)^2) r2 &lt;-ESS/TSS r2 ## [1] 0.1151953 Let’s now show that for any individual observation, TSS_i != ESS_i + RSS_i: sample$TSS_i &lt;-(sample$y-ybar)^2 sample$ESS_i &lt;-(sample$predictedvalues-ybar)^2 sample$RSS_i &lt;- (sample$y-sample$predictedvalues)^2 sample$check_i&lt;-sample$TSS_i-sample$ESS_i-sample$RSS_i summary(sample$check_i) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -39.14458 -2.13315 0.04082 0.00000 2.31464 27.44822 But that it works in aggregate: check_sum&lt;-sum(sample$TSS_i)-sum(sample$ESS_i)-sum(sample$RSS_i) check_sum ## [1] 0 5.8 Algebraic properties of the estimator Show that sum(U) from previous problem =~ 0. Show that sum(X*u)~=0. sum(sample$residuals) ## [1] -1.421085e-13 sample$XU&lt;- sample$residuals*sample$x sum(sample$XU) ## [1] -1.512263e-12 "],
["partialling-out.html", "Chapter 6 Partialling out 6.1 Goals 6.2 Overview 6.3 Generating data from a known linear multivariate DGP 6.4 Sampling from the population 6.5 Calculating sample estimates 6.6 Partialling out: extracting the unique information in \\(x_1\\)", " Chapter 6 Partialling out .badCode { background-color: #ffffcf; color: black; } 6.1 Goals Simulate a data-generating process (DGP) from scratch, given the population parameters Show how to sample from the DGP to generate sample estimates Understand why these sample estimates can be different from the population values Note : Code chunks with STUDENTS are for intented to teach students, CHALLENGE are challenging but students with some programming skills can easily learn, PROF are for students with advanced programming skills and instructors. 6.2 Overview We next make the DGP slightly more complicated – with two independent variables – and look at how our sample estimates are distributed across a series of sample draws from the population data. 6.3 Generating data from a known linear multivariate DGP So let’s perform a data generating process for a dependent variable \\[y\\] which is given by the linear function \\[y = \\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2 + u\\] where \\(\\beta_0\\) is the sum of the 1st and 2nd digit of my birthday, \\(\\beta_1\\) is the sum of the 3rd and 4th digit of my birthday, \\(\\beta_2\\) is 7, \\(x_1\\) is distributed normal with mean of 10 and variance of the first non-zero digit of my birthday, \\(x_2\\) is distributed normal with a mean of the last digit of my birthday, and variance of the sum of the first two digits, and u is (independently) distributed normal with mean 0 and variance of the last non-zero digit of your birthday. Simulate “population” data of 1MM observations. So generate all the components, and then multiply and sum them to generate the column for Y, and combine all columns into a single dataframe. n &lt;- 1000000 u&lt;-rnorm(n, 0, sqrt(9)) x1&lt;-rnorm(n, 10,sqrt(1)) x2&lt;-rnorm(n, 0,sqrt(1)) beta0&lt;-1 beta1&lt;-1 beta2&lt;-7 y&lt;-beta0+beta1*x1+beta2*x2+u df&lt;-data.frame(y,x1,x2,u) head(df) ## y x1 x2 u ## 1 9.341616 10.49637 -0.1952076 -0.7882969 ## 2 13.670843 10.45746 0.1743137 0.9931879 ## 3 19.014744 11.68836 1.4566627 -3.8702541 ## 4 2.591345 10.14413 -1.0416190 -1.2614506 ## 5 16.720966 10.01319 0.7446609 0.4951523 ## 6 18.755255 10.03691 0.1626477 6.5798110 6.4 Sampling from the population Next, we will take a sample from the population data of 1,000 observation. sample&lt;- df[sample(nrow(df), 1000), ] nrow(sample) ## [1] 1000 6.5 Calculating sample estimates And let’s use the lm() command. model1&lt;-lm(sample$y ~ sample$x1 + sample$x2) summary(model1) ## ## Call: ## lm(formula = sample$y ~ sample$x1 + sample$x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.3004 -2.1703 -0.1084 2.0879 10.9532 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.61140 1.02341 0.597 0.55 ## sample$x1 1.03423 0.10131 10.208 &lt;2e-16 *** ## sample$x2 6.89838 0.09914 69.582 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.097 on 997 degrees of freedom ## Multiple R-squared: 0.8325, Adjusted R-squared: 0.8321 ## F-statistic: 2477 on 2 and 997 DF, p-value: &lt; 2.2e-16 model1$coefficients ## (Intercept) sample$x1 sample$x2 ## 0.6113968 1.0342270 6.8983797 beta0_hat&lt;-model1$coefficients[1] beta1_hat&lt;-model1$coefficients[2] beta2_hat&lt;-model1$coefficients[3] So our sample-derived regression function is given by: \\[y = 0.6113968 + 1.034227*x1 + 6.8983797*x2 + u\\] 6.6 Partialling out: extracting the unique information in \\(x_1\\) auxiliary&lt;-lm(sample$x1 ~ sample$x2) summary(auxiliary) ## ## Call: ## lm(formula = sample$x1 ~ sample$x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.70223 -0.63885 -0.00263 0.65125 3.03985 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.055276 0.030603 328.572 &lt;2e-16 *** ## sample$x2 0.005442 0.030976 0.176 0.861 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9677 on 998 degrees of freedom ## Multiple R-squared: 3.092e-05, Adjusted R-squared: -0.0009711 ## F-statistic: 0.03086 on 1 and 998 DF, p-value: 0.8606 sample$residX1 &lt;- residuals(auxiliary) model1PO&lt;-lm(sample$y ~ sample$residX1) summary(model1PO) ## ## Call: ## lm(formula = sample$y ~ sample$residX1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.9156 -5.3660 -0.3198 4.9363 23.8953 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.0256 0.2371 46.508 &lt; 2e-16 *** ## sample$residX1 1.0342 0.2452 4.218 2.69e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.497 on 998 degrees of freedom ## Multiple R-squared: 0.01751, Adjusted R-squared: 0.01653 ## F-statistic: 17.79 on 1 and 998 DF, p-value: 2.693e-05 summary(model1) ## ## Call: ## lm(formula = sample$y ~ sample$x1 + sample$x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.3004 -2.1703 -0.1084 2.0879 10.9532 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.61140 1.02341 0.597 0.55 ## sample$x1 1.03423 0.10131 10.208 &lt;2e-16 *** ## sample$x2 6.89838 0.09914 69.582 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.097 on 997 degrees of freedom ## Multiple R-squared: 0.8325, Adjusted R-squared: 0.8321 ## F-statistic: 2477 on 2 and 997 DF, p-value: &lt; 2.2e-16 "],
["sampling-distribution.html", "Chapter 7 Sampling distribution 7.1 Goals 7.2 Overview 7.3 Generating data from a known linear univariate DGP 7.4 Sampling from the population 7.5 Properties of the empirical simulations 7.6 Changing the error variance", " Chapter 7 Sampling distribution .badCode { background-color: #ffffcf; color: black; } 7.1 Goals Simulate a data-generating process (DGP) from scratch, given the population parameters Show how to sample from the DGP numerous times, with each sample generating a sample estimate SHow that these sets of sampling estimates follow a distribution Note : Code chunks with STUDENTS are for intented to teach students, CHALLENGE are challenging but students with some programming skills can easily learn, PROF are for students with advanced programming skills and instructors. 7.2 Overview We follow the univariate DGP from before. We then loop our process to generate many samples, each with its own estimate of \\(\\beta_1\\). We then graph the distribution of the estimates and look at their properties. 7.3 Generating data from a known linear univariate DGP So let’s perform a data generating process for a dependent variable \\[y\\] which is given by the linear function \\[y = \\beta_0 + \\beta_1 * x + u\\] where \\(\\beta_0\\) is the sum of the 1st and 2nd digit of my birthday, \\(\\beta_1\\) is the sum of the 3rd and 4th digit of my birthday, x is distributed normal with mean of 10 and variance of the first non-zero digit of my birthday, and u is (independently) distributed normal with mean 0 and variance of the last non-zero digit of your birthday. Let’s say the “population” is 1MM observations. So generate all the components, and then multiply and sum them to generate the column for Y, and combine all columns into a single dataframe. n &lt;- 1000000 u&lt;-rnorm(n, 0, sqrt(9)) x&lt;-rnorm(n, 10,sqrt(1)) beta0&lt;-1 beta1&lt;-1 y&lt;-beta0+beta1*x+u df&lt;-data.frame(y,x,u) head(df) ## y x u ## 1 10.708070 10.49637 -0.7882969 ## 2 12.450647 10.45746 0.9931879 ## 3 8.818106 11.68836 -3.8702541 ## 4 9.882678 10.14413 -1.2614506 ## 5 11.508339 10.01319 0.4951523 ## 6 17.616721 10.03691 6.5798110 7.4 Sampling from the population Next, we will take 1,000 samples of 1,000 observation from the population data, with each sample yielding us a \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). b_0&lt;-c() b_1&lt;-c() for(i in 1:1000){ sff&lt;- df[sample(nrow(df), 1000, replace=FALSE), ] estimates&lt;-lm(y~x, sff) b_0[i]&lt;-coef(estimates)[1] b_1[i]&lt;-coef(estimates)[2] } coeff&lt;-data.frame(b_0, b_1) head(coeff) ## b_0 b_1 ## 1 0.5865453 1.0408402 ## 2 1.4482701 0.9514253 ## 3 0.9756452 1.0056688 ## 4 -0.0979289 1.1108554 ## 5 2.5829476 0.8484714 ## 6 0.4448188 1.0627064 ggplot(coeff, aes(x=b_0)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, binwidth = 0.05)+ ggtitle(&#39;TKtitle&#39;) ggplot(coeff, aes(x=b_1)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, binwidth = 0.05)+ ggtitle(&#39;TKtitle&#39;) 7.5 Properties of the empirical simulations mean(coeff$b_0) ## [1] 0.9607526 mean(coeff$b_1) ## [1] 1.003667 var(coeff$b_0) ## [1] 0.9214121 var(coeff$b_1) ## [1] 0.009174074 7.6 Changing the error variance u50&lt;-rnorm(n, 0, sqrt(50)) u1&lt;-rnorm(n, 0, sqrt(1)) y2&lt;-beta0+beta1*x+u50 y3&lt;-beta0+beta1*x+u1 df&lt;-data.frame(y,y2,y3,x,u) head(df) ## y y2 y3 x u ## 1 10.708070 10.123984 11.822746 10.49637 -0.7882969 ## 2 12.450647 11.042329 13.280761 10.45746 0.9931879 ## 3 8.818106 8.580325 11.977656 11.68836 -3.8702541 ## 4 9.882678 8.961635 9.944481 10.14413 -1.2614506 ## 5 11.508339 5.451122 9.761655 10.01319 0.4951523 ## 6 17.616721 13.849249 10.486425 10.03691 6.5798110 b_0_u50&lt;-c() b_1_u50&lt;-c() b_0_u1&lt;-c() b_1_u1&lt;-c() for(i in 1:1000){ sff&lt;- df[sample(nrow(df), 1000, replace=FALSE), ] estimates50&lt;-lm(y2~x, sff) b_0_u50[i]&lt;-coef(estimates50)[1] b_1_u50[i]&lt;-coef(estimates50)[2] estimates1&lt;-lm(y3~x, sff) b_0_u1[i]&lt;-coef(estimates1)[1] b_1_u1[i]&lt;-coef(estimates1)[2] } coeff&lt;-data.frame(b_0_u50, b_1_u50, b_0_u1, b_1_u1) head(coeff) ## b_0_u50 b_1_u50 b_0_u1 b_1_u1 ## 1 -0.5737496 1.1367457 0.8919822 1.0120075 ## 2 -1.6615207 1.2792027 1.2172898 0.9790788 ## 3 0.5631146 1.0546185 1.3297850 0.9680056 ## 4 5.4433421 0.5786502 1.0783725 0.9893487 ## 5 -1.8864498 1.2727719 1.2892258 0.9722462 ## 6 0.4815144 1.0847731 0.8847833 1.0125480 ggplot(coeff, aes(x=b_1_u50)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, binwidth = 0.05)+ ggtitle(&#39;TKtitle&#39;) ggplot(coeff, aes(x=b_1_u1)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, binwidth = 0.05)+ ggtitle(&#39;TKtitle&#39;) "],
["hypothesis-testing.html", "Chapter 8 Hypothesis testing 8.1 Goals (previous) 8.2 Goals (current) 8.3 Overview 8.4 Generating data from a known linear univariate DGP 8.5 Sampling from the population 8.6 Properties of the empirical simulations", " Chapter 8 Hypothesis testing .badCode { background-color: #ffffcf; color: black; } 8.1 Goals (previous) Simulate a data-generating process (DGP) from scratch, given the population parameters Show how to sample from the DGP numerous times, with each sample generating a sample estimate Show that these sets of sampling estimates follow a distribution Note : Code chunks with STUDENTS are for intented to teach students, CHALLENGE are challenging but students with some programming skills can easily learn, PROF are for students with advanced programming skills and instructors. 8.2 Goals (current) Show a sampling distribution when true \\(\\beta_1 = 0\\). 8.3 Overview We follow the univariate DGP from before. We then loop our process to generate many samples, each with its own estimate of \\(\\beta_1\\). We then graph the distribution of the estimates and look at their properties. The only change here is to set \\(\\beta_1= 0\\) in the DGP. 8.4 Generating data from a known linear univariate DGP n &lt;- 1000000 u&lt;-rnorm(n, 0, sqrt(9)) x&lt;-rnorm(n, 10,sqrt(1)) beta0&lt;-1 beta1&lt;-0 y&lt;-beta0+beta1*x+u df&lt;-data.frame(y,x,u) head(df) ## y x u ## 1 0.2117031 10.49637 -0.7882969 ## 2 1.9931879 10.45746 0.9931879 ## 3 -2.8702541 11.68836 -3.8702541 ## 4 -0.2614506 10.14413 -1.2614506 ## 5 1.4951523 10.01319 0.4951523 ## 6 7.5798110 10.03691 6.5798110 8.5 Sampling from the population Next, we will take 1,000 samples of 1,000 observation from the population data, with each sample yielding us a \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). b_0&lt;-c() b_1&lt;-c() for(i in 1:1000){ sff&lt;- df[sample(nrow(df), 1000, replace=FALSE), ] estimates&lt;-lm(y~x, sff) b_0[i]&lt;-coef(estimates)[1] b_1[i]&lt;-coef(estimates)[2] } coeff&lt;-data.frame(b_0, b_1) head(coeff) ## b_0 b_1 ## 1 0.5865453 0.040840194 ## 2 1.4482701 -0.048574735 ## 3 0.9756452 0.005668765 ## 4 -0.0979289 0.110855352 ## 5 2.5829476 -0.151528630 ## 6 0.4448188 0.062706428 ggplot(coeff, aes(x=b_0)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, binwidth = 0.05)+ ggtitle(&#39;TKtitle&#39;) ggplot(coeff, aes(x=b_1)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, binwidth = 0.05)+ ggtitle(&#39;TKtitle&#39;) 8.6 Properties of the empirical simulations mean(coeff$b_0) ## [1] 0.9607526 mean(coeff$b_1) ## [1] 0.003667223 var(coeff$b_0) ## [1] 0.9214121 var(coeff$b_1) ## [1] 0.009174074 8.6.1 What values of B1 hat define the cutoff for the 5% of observations in either tail? quantile(coeff$b_1, c(.05, .95)) ## 5% 95% ## -0.1492310 0.1640644 8.6.2 Divide all values of B1 hat by the standard deviation of the distribution of b1 hats. What value of this transformed B1 hat defines the cutoff for the 5% of observations with the largest absolute value you get in the (transformed) sampling distribution? coeff$b_1_transformed &lt;- coeff$b_1/sd(coeff$b_1) quantile(coeff$b_1_transformed, c(.05, .95)) ## 5% 95% ## -1.558038 1.712905 8.6.3 Randomly sample coefficients from the distribution until you get one that is in the 5% tails. How many tries does it take to get a value “far away” from the true value? tails&lt;-quantile(coeff$b_1, c(.05, .95)) sample_beta1&lt;-0 #start by setting this to zero for the while loop i&lt;-1 #loop counter for print while (sample_beta1&gt;=tails[1] &amp; sample_beta1&lt;=tails[2]) { sample_beta1&lt;-sample(coeff$b_1,1) print(paste0(&#39;Sample number &#39;,i,&quot;: &quot;,sample_beta1)) i&lt;-i+1 } ## [1] &quot;Sample number 1: 0.034078008449304&quot; ## [1] &quot;Sample number 2: -0.060571914180609&quot; ## [1] &quot;Sample number 3: -0.0112119751697641&quot; ## [1] &quot;Sample number 4: -0.0701335192419715&quot; ## [1] &quot;Sample number 5: -0.0342942520155939&quot; ## [1] &quot;Sample number 6: -0.0560707104786931&quot; ## [1] &quot;Sample number 7: 0.079284782714954&quot; ## [1] &quot;Sample number 8: 0.0401660398633941&quot; ## [1] &quot;Sample number 9: 0.053062886886849&quot; ## [1] &quot;Sample number 10: -0.311000129203764&quot; "],
["consistency.html", "Chapter 9 Consistency 9.1 Goals (previous) 9.2 Goals (current) 9.3 Overview 9.4 Generating data from a known linear univariate DGP 9.5 Sampling from the population 9.6 Properties of the empirical simulations", " Chapter 9 Consistency .badCode { background-color: #ffffcf; color: black; } 9.1 Goals (previous) Simulate a data-generating process (DGP) from scratch, given the population parameters Show how to sample from the DGP numerous times, with each sample generating a sample estimate Show that these sets of sampling estimates follow a distribution Note : Code chunks with STUDENTS are for intented to teach students, CHALLENGE are challenging but students with some programming skills can easily learn, PROF are for students with advanced programming skills and instructors. 9.2 Goals (current) Show a sampling distribution when true \\(\\beta_1 = 0\\). 9.3 Overview We follow the univariate DGP from before. We then loop our process to generate many samples, each with its own estimate of \\(\\beta_1\\). We then graph the distribution of the estimates and look at their properties. The only change here is to set \\(\\beta_1= 0\\) in the DGP. In this problem set, you generated sampling distributions of (β_1 ) ̂ when U ~ N(0,5), and were taking 1,000 samples of 1,000 observations each. Construct a “population” dataset with 1,000,000 observations with this same DGP (all other parameters of the setup stay the same). Take 1,000 samples (and get the empirical variance of (β_1 ) ̂) based on sampling 10,, 100, 1,000, and 10,000 observations each time (so, 1,000 samples of 10 observations, 1000 samples of 100 observations, etc.). Plot the 4 histograms (or density plots) that result from these simulations. Either place all 4 histograms on the same plot, or make the x-axis on all 4 the same range. Make another plot, which on the x-axis has the sample size you were using, and on the y-axis you have the empirical variance across the 1,000 draws. Advanced-extra credit on this PS. Iterate through equally-spaced bins of sample size (instead of 10, 100, 1000, etc. start at 500, then 1000, then 1500, up to some high number) so that you can get the shape of the curve in part (c) instead of just 4 (x,y) points to plot. 9.4 Generating data from a known linear univariate DGP n &lt;- 1000000 u&lt;-rnorm(n, 0, sqrt(9)) x&lt;-rnorm(n, 10,sqrt(1)) beta0&lt;-1 beta1&lt;-0 y&lt;-beta0+beta1*x+u df&lt;-data.frame(y,x,u) head(df) ## y x u ## 1 0.2117031 10.49637 -0.7882969 ## 2 1.9931879 10.45746 0.9931879 ## 3 -2.8702541 11.68836 -3.8702541 ## 4 -0.2614506 10.14413 -1.2614506 ## 5 1.4951523 10.01319 0.4951523 ## 6 7.5798110 10.03691 6.5798110 9.5 Sampling from the population Next, we will take 1,000 samples of 1,000 observation from the population data, with each sample yielding us a \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). b_0&lt;-c() b_1&lt;-c() for(i in 1:1000){ sff&lt;- df[sample(nrow(df), 1000, replace=FALSE), ] estimates&lt;-lm(y~x, sff) b_0[i]&lt;-coef(estimates)[1] b_1[i]&lt;-coef(estimates)[2] } coeff&lt;-data.frame(b_0, b_1) head(coeff) ## b_0 b_1 ## 1 0.5865453 0.040840194 ## 2 1.4482701 -0.048574735 ## 3 0.9756452 0.005668765 ## 4 -0.0979289 0.110855352 ## 5 2.5829476 -0.151528630 ## 6 0.4448188 0.062706428 ggplot(coeff, aes(x=b_0)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, binwidth = 0.05)+ ggtitle(&#39;TKtitle&#39;) ggplot(coeff, aes(x=b_1)) + geom_histogram(color=&quot;black&quot;, fill=&quot;pink&quot;, binwidth = 0.05)+ ggtitle(&#39;TKtitle&#39;) 9.6 Properties of the empirical simulations mean(coeff$b_0) ## [1] 0.9607526 mean(coeff$b_1) ## [1] 0.003667223 var(coeff$b_0) ## [1] 0.9214121 var(coeff$b_1) ## [1] 0.009174074 9.6.1 What values of B1 hat define the cutoff for the 5% of observations in either tail? quantile(coeff$b_1, c(.05, .95)) ## 5% 95% ## -0.1492310 0.1640644 9.6.2 Divide all values of B1 hat by the standard deviation of the distribution of b1 hats. What value of this transformed B1 hat defines the cutoff for the 5% of observations with the largest absolute value you get in the (transformed) sampling distribution? coeff$b_1_transformed &lt;- coeff$b_1/sd(coeff$b_1) quantile(coeff$b_1_transformed, c(.05, .95)) ## 5% 95% ## -1.558038 1.712905 9.6.3 Randomly sample coefficients from the distribution until you get one that is in the 5% tails. How many tries does it take to get a value “far away” from the true value? tails&lt;-quantile(coeff$b_1, c(.05, .95)) sample_beta1&lt;-0 #start by setting this to zero for the while loop i&lt;-1 #loop counter for print while (sample_beta1&gt;=tails[1] &amp; sample_beta1&lt;=tails[2]) { sample_beta1&lt;-sample(coeff$b_1,1) print(paste0(&#39;Sample number &#39;,i,&quot;: &quot;,sample_beta1)) i&lt;-i+1 } ## [1] &quot;Sample number 1: 0.034078008449304&quot; ## [1] &quot;Sample number 2: -0.060571914180609&quot; ## [1] &quot;Sample number 3: -0.0112119751697641&quot; ## [1] &quot;Sample number 4: -0.0701335192419715&quot; ## [1] &quot;Sample number 5: -0.0342942520155939&quot; ## [1] &quot;Sample number 6: -0.0560707104786931&quot; ## [1] &quot;Sample number 7: 0.079284782714954&quot; ## [1] &quot;Sample number 8: 0.0401660398633941&quot; ## [1] &quot;Sample number 9: 0.053062886886849&quot; ## [1] &quot;Sample number 10: -0.311000129203764&quot; "],
["references.html", "References", " References "]
]
